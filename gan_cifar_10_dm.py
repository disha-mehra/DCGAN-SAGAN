# -*- coding: utf-8 -*-
"""GAN-CIFAR-10_dm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/disha-mehra/DCGAN-SAGAN/blob/master/GAN_CIFAR_10_dm.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import sys
import os

# %matplotlib inline
import matplotlib.pyplot as plt

from keras.datasets import cifar10
from keras.models import Sequential, Model
from keras.layers import Input, Dense, LeakyReLU, BatchNormalization, ReLU
from keras.layers import Conv2D, Conv2DTranspose, Reshape, Flatten
from keras.optimizers import Adam
from keras import initializers
from keras.utils import plot_model, np_utils
from keras import backend as K

# load dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

fig = plt.figure(figsize=(10,5))
for i in range(0, 50):
    plt.subplot(5, 10, 1 + i, xticks=[], yticks=[])
    plt.imshow(X_train[i])
    
plt.show()

num_classes = len(np.unique(y_train))
class_names = ['airplane','automobile','bird','cat','deer',
               'dog','frog','horse','ship','truck']

fig = plt.figure(figsize=(10,5))
for i in range(num_classes):
    ax = plt.subplot(1, 10, 1 + i, xticks=[], yticks=[])
    idx = np.where(y_train[:]==i)[0]
    features_idx = X_train[idx,::]
    img_num = np.random.randint(features_idx.shape[0])
    img = features_idx[img_num,::]
    ax.set_title(class_names[i])
    plt.imshow(img)
    
plt.tight_layout()

print('X_train shape:', X_train.shape)
print('X_test_shape:' , X_test.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

X_test.shape

if K.image_data_format() == 'channels_first':
    X_train = X_train.reshape(X_train.shape[0], 3, 32, 32)
    X_test = X_test.reshape(X_test.shape[0], 3, 32, 32)
    input_shape = (3, 32, 32)
else:
    X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)
    X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)
    input_shape = (32, 32, 3)
    
# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, num_classes)
Y_test = np_utils.to_categorical(y_test, num_classes)

# the generator is using tanh activation, for which we need to preprocess 
# the image data into the range between -1 and 1.

X_train = np.float32(X_train)
X_train = (X_train / 255 - 0.5) * 2
X_train = np.clip(X_train, -1, 1)

X_test = np.float32(X_test)
X_test = (X_test / 255 - 0.5) * 2
X_test = np.clip(X_test, -1, 1)

print('X_train reshape:', X_train.shape)
print('X_test reshape:', X_test.shape)

"""## **Model definition**
### Generator
"""

# latent space dimension
latent_dim = 100

init = initializers.RandomNormal(stddev=0.002)

# Generator network
generator = Sequential()

# Can't follow the exact architecture of the paper as the dataset is different and one has to accomodate the sixe of the outputs based on what size of image one needs at the end
# FC: 2x2x512
generator.add(Dense(2*2*512, input_shape=(latent_dim,), kernel_initializer=init))#The generator will take as input arrays of shape (,100) and output arrays of shape (2,2,512)
generator.add(Reshape((2, 2, 512))) 
generator.add(BatchNormalization())
generator.add(ReLU(0.2))


# Conv 1: 4x4x256
generator.add(Conv2DTranspose(256, kernel_size=5, strides=(2,2), padding='same')) #dimensionality of output space, size of the kernel
generator.add(BatchNormalization()) 
generator.add(ReLU(0.2))

# Conv 2: 8x8x128
generator.add(Conv2DTranspose(128, kernel_size=5, strides=(2,2), padding='same')) #dimensionality of output space, size of the kernel
generator.add(BatchNormalization()) 
generator.add(ReLU(0.2))

# Conv 3: 16x16x64
generator.add(Conv2DTranspose(64, kernel_size=5, strides=(2,2), padding='same'))
generator.add(BatchNormalization())
generator.add(ReLU(0.2))

# Conv 4: 32x32x3
generator.add(Conv2DTranspose(3, kernel_size=5, strides=(2,2), padding='same',
                              activation='tanh'))

generator.summary()

"""## **Discriminator**"""

# image shape 32x32x3
img_shape = X_train[0].shape

# Discriminator network
discriminator = Sequential()

# Conv 1: 16x16x64
discriminator.add(Conv2D(64, kernel_size=5, strides=2, padding='same',
                         input_shape=(img_shape), kernel_initializer=init))
discriminator.add(LeakyReLU(0.2))

# Conv 2:
discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))
discriminator.add(BatchNormalization())
discriminator.add(LeakyReLU(0.2))

# Conv 3: 
discriminator.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))
discriminator.add(BatchNormalization())
discriminator.add(LeakyReLU(0.2))

# Conv 3: 
discriminator.add(Conv2D(512, kernel_size=5, strides=2, padding='same'))
discriminator.add(BatchNormalization())
discriminator.add(LeakyReLU(0.2))

# FC
discriminator.add(Flatten())

# Output
discriminator.add(Dense(1, activation='sigmoid'))

discriminator.summary()

"""### **Compile** **Discriminator**"""

# Wasserstein objective
def wasserstein_loss(y_true, y_pred):
    return K.mean(y_true * y_pred)

# Following parameter and optimizer set as recommended in paper
# optimizer = Adam(lr=0.0002)


# discriminator.compile(Adam(lr=0.0003, beta_1=0.5), loss='binary_crossentropy',
#                       metrics=['binary_accuracy'])

discriminator.compile(Adam(lr=0.0005, beta_1=0.5), loss=wasserstein_loss, metrics=['accuracy'])

"""### **Combined** **Network**"""

# d_g = discriminador(generador(z))
discriminator.trainable = False

z = Input(shape=(latent_dim,))
img = generator(z)

# The discriminator takes generated images as input and determines validity
results = discriminator(img)

# The combined model (discriminator and generative)
d_g = Model(inputs=z, outputs=results, name='wgan')

d_g.compile(Adam(lr=0.0005, beta_1=0.5), loss=wasserstein_loss,
            metrics=['accuracy'])

"""### **GAN model visualization**"""

d_g.summary()

"""### **Fit** **Model**"""

epochs = 100
batch_size = 32
smooth = 0.1
n_discriminator=5
clip_value = 0.01

real = np.ones(shape=(batch_size, 1))
fake = np.zeros(shape=(batch_size, 1))

d_loss = []
g_loss = []

for e in range(epochs + 1):
    for i in range(len(X_train) // batch_size):
        for _ in range(n_discriminator):            
             # Train Discriminator weights
            discriminator.trainable = True

            # Real samples
            X_batch = X_train[i*batch_size:(i+1)*batch_size]
            d_loss_real = discriminator.train_on_batch(x=X_batch,
                                                       y=real * (1 - smooth))

            # Fake Samples
            z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))
            X_fake = generator.predict_on_batch(z)
            d_loss_fake = discriminator.train_on_batch(x=X_fake, y=fake)

            # Discriminator loss
            d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])
            
            # Clip discriminator weights
            for l in discriminator.layers:
                weights = l.get_weights()
                weights = [np.clip(w, -clip_value, clip_value) for w in weights]
                l.set_weights(weights)


        # Train Generator weights
        discriminator.trainable = False
        g_loss_batch = d_g.train_on_batch(x=z, y=real)

        print(
            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, g_loss_batch[0]),
            100*' ',
            end='\r'
        )

    d_loss.append(d_loss_batch)
    g_loss.append(g_loss_batch[0])
    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], g_loss[-1]), 100*' ')

    if e % 10 == 0:
        samples = 10
        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, latent_dim)))

        for k in range(samples):
            plt.subplot(2, 5, k + 1)
            plt.imshow(((x_fake[k] + 1)* 127).astype(np.uint8))
            plt.xticks([])
            plt.yticks([])

        plt.tight_layout()
        plt.show()

